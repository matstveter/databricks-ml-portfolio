{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d268d1b2-0962-42cd-b8b3-5f064da11348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when, trim, lit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae02be1-51f9-490f-83c2-515661db9f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"/databricks-datasets/adult/adult.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed073eae-56d2-4407-b513-12b4fbe6e093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "df = df_raw.toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2313cfd8-826a-4e1b-9e7e-8bb39e89f601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"label\", when(col(\"income\").contains(\">50K\"), 1).otherwise(0))\n",
    "for c in df.columns:\n",
    "    if dict(df.dtypes)[c] == \"string\":\n",
    "        df = df.withColumn(c, when(trim(col(c)).isin([\"?\", \"\"]), None).otherwise(col(c)))\n",
    "\n",
    "df_clean = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e39fb6d-4981-4375-9eee-7688524d6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Since we are using RF, we can keep native_country as is. \n",
    "# We remove education, as this information is in the eduction_num\n",
    "df_clean = df_clean.drop(\"education\")\n",
    "df_clean = df_clean.drop(\"native_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f5575a-6879-49a3-83b5-adea7433b41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train Rows: {train.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28c0c14-782e-4549-b6a1-468d8424b3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_census_model(depth, trees):\n",
    "\n",
    "    with mlflow.start_run(run_name=\"census_rf_model\") as run:\n",
    "        mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "        mlflow.log_param(\"max_depth\", depth)\n",
    "        mlflow.log_param(\"num_trees\" , trees)\n",
    "\n",
    "        stages = []\n",
    "        cat_cols = [\"workclass\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\"]\n",
    "        num_cols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "\n",
    "        for c in cat_cols:\n",
    "            stages.append(StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"skip\"))\n",
    "            \n",
    "        final_inputs = [c+\"_idx\" for c in cat_cols] + num_cols\n",
    "        stages.append(VectorAssembler(inputCols=final_inputs, outputCol=\"features\", handleInvalid=\"skip\"))\n",
    "        \n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=depth, numTrees=trees, seed=42, maxBins=42)\n",
    "        stages.append(rf)\n",
    "\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "\n",
    "        print(\"Training Pipeline...\")\n",
    "        model = pipeline.fit(train)\n",
    "\n",
    "        try:\n",
    "            local_path = \"/tmp/census_rf_model\"\n",
    "            if os.path.exists(local_path):\n",
    "                shutil.rmtree(local_path) # Clean up old run\n",
    "            \n",
    "            # Save to local linux filesystem\n",
    "            model.write().overwrite().save(f\"file:{local_path}\")\n",
    "            \n",
    "            # Log as generic artifact (Bypasses UC Volume check)\n",
    "            mlflow.log_artifacts(local_path, artifact_path=\"spark_model\")\n",
    "            print(\"Model saved via Local Workaround.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Model Saving Failed (Expected on some clusters): {e}\")\n",
    "\n",
    "        # Predict\n",
    "        predictions = model.transform(test)\n",
    "\n",
    "        eval_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "        eval_auc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "        \n",
    "        f1 = eval_f1.evaluate(predictions)\n",
    "        auc = eval_auc.evaluate(predictions)\n",
    "\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        print(f\"F1:  {f1:.4f}\")\n",
    "\n",
    "        # Log the metrics in mlflow\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"auc\", auc)\n",
    "\n",
    "        try:\n",
    "            rf_model = model.stages[-1]\n",
    "            importances = rf_model.featureImportances.toArray()\n",
    "            \n",
    "            # Reconstruct Names manually (Lite Pipeline = Direct Mapping)\n",
    "            feat_names = [c+\"_idx\" for c in cat_cols] + num_cols\n",
    "            \n",
    "            # Create DataFrame\n",
    "            fi_df = pd.DataFrame({'Feature': feat_names, 'Importance': importances}).sort_values(by='Importance', ascending=False).head(15)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.barh(fi_df['Feature'], fi_df['Importance'], color='teal')\n",
    "            plt.title(f\"Feature Importance (Depth {depth})\")\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save & Log\n",
    "            plot_path = \"/tmp/feature_importance.png\"\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(plot_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting failed: {e}\")\n",
    "        finally:\n",
    "            del model\n",
    "\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18e525e-1de2-4441-b644-4fb0455c37e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id = train_census_model(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a459c2f8-5c33-463d-9115-3048a4b64b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow_census",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
